{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertForMaskedLM(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-21): 21 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=768, out_features=50368, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"The capital of Germany is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Predicted token:  Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to modernbert.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input tensors matching the expected input shapes\n",
    "batch_size = 1\n",
    "sequence_length = inputs[\"input_ids\"].shape[1]  # Get sequence length from previous example\n",
    "dummy_input_ids = torch.zeros((batch_size, sequence_length), dtype=torch.int64)\n",
    "dummy_attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.int64)\n",
    "\n",
    "# Define dynamic axes for variable sequence lengths\n",
    "dynamic_axes = {\n",
    "    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "    'output': {0: 'batch_size', 1: 'sequence_length'}\n",
    "}\n",
    "\n",
    "# Export the model to ONNX\n",
    "output_path = \"modernbert.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,                                         # Model being exported\n",
    "    (                                              # Model input args \n",
    "        dummy_input_ids,                           # Input ids\n",
    "        dummy_attention_mask,                      # Attention mask\n",
    "        None,                                      # token_type_ids (not used)\n",
    "        None,                                      # position_ids (not used) \n",
    "        None,                                      # inputs_embeds (not used)\n",
    "        None,                                      # labels (not used)\n",
    "        None,                                      # output_attentions (not used)\n",
    "        None,                                      # output_hidden_states (not used)\n",
    "        None,                                      # return_dict (not used)\n",
    "    ),                                            \n",
    "    output_path,                                   # Output file path\n",
    "    input_names=['input_ids', 'attention_mask'],   # Input names\n",
    "    output_names=['output'],                       # Output names\n",
    "    dynamic_axes=dynamic_axes,                     # Dynamic axes specification\n",
    "    opset_version=20,                             # ONNX opset version\n",
    "    do_constant_folding=True,                     # Fold constants for optimization\n",
    ")\n",
    "\n",
    "print(f\"Model exported to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime Prediction: [CLS]The capital of Germany is Berlin.[SEP]\n",
      "PyTorch Model Prediction: [CLS]The capital of Germany is Berlin.[SEP]\n",
      "✓ ONNX Runtime and PyTorch outputs match within tolerance\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Initialize ONNX Runtime session\n",
    "ort_session = onnxruntime.InferenceSession(\"modernbert.onnx\")\n",
    "\n",
    "# Convert the PyTorch tensors to numpy arrays for ONNX Runtime\n",
    "ort_inputs = {\n",
    "    'input_ids': inputs['input_ids'].numpy(),\n",
    "    'attention_mask': inputs['attention_mask'].numpy()\n",
    "}\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "ort_outputs = ort_session.run(['output'], ort_inputs)\n",
    "\n",
    "# Get the output logits\n",
    "ort_logits = ort_outputs[0]\n",
    "\n",
    "# Convert to probabilities using softmax\n",
    "ort_probs = np.exp(ort_logits) / np.sum(np.exp(ort_logits), axis=-1, keepdims=True)\n",
    "\n",
    "# Get the predicted class (highest probability)\n",
    "ort_predicted = np.argmax(ort_probs, axis=-1)\n",
    "\n",
    "print(\"ONNX Runtime Prediction:\", tokenizer.decode(ort_predicted[0]))\n",
    "\n",
    "# Compare with PyTorch model output (optional)\n",
    "with torch.no_grad():\n",
    "    torch_outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    torch_probs = torch.softmax(torch_outputs.logits, dim=-1)\n",
    "    torch_predicted = torch.argmax(torch_probs, dim=-1)\n",
    "\n",
    "print(\"PyTorch Model Prediction:\", tokenizer.decode(torch_predicted[0]))\n",
    "\n",
    "# Verify the outputs match\n",
    "np.testing.assert_allclose(ort_probs, torch_probs.numpy(), rtol=1e-3, atol=1e-3)\n",
    "print(\"✓ ONNX Runtime and PyTorch outputs match within tolerance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
